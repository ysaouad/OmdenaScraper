{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5e004f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, lxml, cchardet, re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a0991",
   "metadata": {},
   "source": [
    "Picked 'user-agent' for my browser from https://www.whatismybrowser.com/detect/what-is-my-user-agent/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a83099",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://patient.info/forums/discuss/browse/depression-683\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "    #Switch the User-agent to avoid anti-scraping measures and \"make believe\" this is a real human requests those URLs\n",
    "}\n",
    "session_object = requests.Session() \n",
    "#Update : use a session to avoid making one request per page + \n",
    "#          saving time& the server's ressources\n",
    "response = session_object.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'lxml') #using lxml to parse the main page, saving time\n",
    "\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbc8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3f076",
   "metadata": {},
   "source": [
    "#### This part is only for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04820759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dom = etree.HTML(str(soup)) #turning the parsed HTML into a dom.xpath object that can be selected using xPath\n",
    "\n",
    "#for j in range(1, 37):\n",
    "#try:\n",
    "#        print('https://patient.info'+dom.xpath(f'/html/body/div/div/div[2]/div[1]/div/div/div/div[1]/div/ul/li[{j}]/div/div[2]/article/h3/a/@href')[0])\n",
    "#    except:\n",
    "#        print(dom.xpath(f'/html/body/div/div/div[2]/div[1]/div/div/div/div[1]/div/ul/li[{j}]/div/div[2]/article/h3/a/@href'), \"couldn't be found\")\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1835a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOP THROUGH EACH PAGE FROM 1 TO 198 starting with : \n",
    "### https://patient.info/forums/discuss/browse/depression-683?page=0#group-discussions LOOP AROUND PAGE = 0 \n",
    "### then PAGE = 1 then PAGE = 2...up to 197\n",
    "#import logging \n",
    "\n",
    "post_urls = []\n",
    "post_url = 0\n",
    "\n",
    "for i in range(): \n",
    "    \n",
    "    #USE len(198) in range() for a FULL crawl. Do not crawl the entire forum unless necessary. \n",
    "    #The final results are on github. If you want to test the code, simply use a range(1,2)\n",
    "    \n",
    "    page_url = f\"https://patient.info/forums/discuss/browse/depression-683?page={i}#group-discussions\"\n",
    "    response = session_object.get(page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    " ### FOR EACH TOPIC NAME (use XPATH) GRAB URL AND APPEND TO A LIST   \n",
    "    for j in range(1, 37):\n",
    "        XPATH = f\"/html/body/div/div/div[2]/div[1]/div/div/div/div[1]/div/ul/li[{j}]/div/div[2]/article/h3/a/@href\"\n",
    "        \n",
    "        dom = etree.HTML(str(soup))\n",
    "        try:\n",
    "            post_url = 'https://patient.info' + dom.xpath(XPATH)[0]\n",
    "            post_urls.append(post_url)\n",
    "        except:\n",
    "            #logging.exception(\"An exception was thrown!\")\n",
    "            #print(i, \"i\", page_url)\n",
    "            #print(post_url, \"couldn't be scraped\")\n",
    "            \n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57138ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://patient.info/forums/discuss/depression-fake-illnes--11369',\n",
       " 'https://patient.info/forums/discuss/citralpram-8895',\n",
       " 'https://patient.info/forums/discuss/i-suffered-with-postnatal-depression-after-the-birth-of--4901',\n",
       " 'https://patient.info/forums/discuss/i-d-like-to-hear-from-anyone-with-similar-experience-to--6919',\n",
       " 'https://patient.info/forums/discuss/i-suffer-with-panic-attacks-since-taking-dothapax-i-have--838']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_urls[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2790530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THEN LOOP THROUGH EVERY URL IN LIST, REQUEST URL, AND SCRAP MAIN POST'S TEXT\n",
    "\n",
    "XPATH = f'/html/body/div/div/div[2]/div[1]/div/div/div/div[1]/article/div[3]/input'\n",
    "post_dict = []\n",
    "fieldnames = [\"URL\",\"Main_Post\"]\n",
    "\n",
    "for i in range(): \n",
    "    #USE len(post_urls) in range() for a FULL crawl. Do not crawl the entire URL list unless necessary. \n",
    "    #The final results are on github. If you want to test the code, simply use a range(1,5)\n",
    "\n",
    "    response = session_object.get(post_urls[i], headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    website = etree.HTML(str(soup))\n",
    "    try:\n",
    "        e = website.xpath('.//*[@id=\"post_content\"]/input')[0]\n",
    "        text = e.attrib[\"value\"]\n",
    "        text = re.sub(r\"\\r\", \"\", text)\n",
    "        text = re.sub(r\"\\n\", \"\", text)\n",
    "        text = re.sub(r\"<p>\", \"\", text)\n",
    "        text = re.sub(r\"</p>\", \"\", text)\n",
    "        post_dict.append({\"URL\":post_urls[i], \"Main_Post\":text})\n",
    "    except: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "969294e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "  \n",
    "with open('Results.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames,lineterminator = '\\n', delimiter = \";\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(post_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
